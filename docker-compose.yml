version: '3.9'

services:
  vllm:
    image: vllm/vllm-openai:v0.6.5
    container_name: vllm
    shm_size: '8g'
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - /home/nextai_2/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8000:8000"
    command: >
      --model Qwen/Qwen2-VL-2B-Instruct
      --dtype bfloat16
      --gpu-memory-utilization 0.98
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 5

  fastapi:
    image: fastapi_server:1.0
    container_name: fastapi
    ports:
      - "8001:8001"
    volumes:
      - /home/nextai_2/.cache/huggingface:/root/.cache/huggingface
    depends_on:
      vllm:
        condition: service_healthy
    command: uvicorn main:app --host 0.0.0.0 --port 8001

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6633:6333"
    volumes:
      - ${PWD}/qdrant_data:/qdrant/storage

  # nginx:
  #   image: nginx:latest
  #   container_name: nginx_load_balancer
  #   ports:
  #     - "8080:80"
  #   volumes:
  #     - ./nginx.conf:/etc/nginx/nginx.conf:ro
  #   depends_on:
  #     vllm1:
  #       condition: service_healthy